{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Bayesian_NN_Example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/arturzeitler/Bayes-and-MC/blob/master/Bayesian_NN_Example.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "LZEqt4-xfdwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A Bayesian neural network is a neural network with a prior distribution on its weights (Neal, 2012).\n",
        "\n",
        "**Regression Model Example:**\n",
        "\n",
        "Consider a data set $\\{(\\mathbf{x}_n, y_n)\\}$, where each data point comprises of features $\\mathbf{x}_n\\in\\mathbb{R}^D$ and output $y_n\\in\\mathbb{R}$. Define the likelihood for each data point as\n",
        "\\begin{aligned} p(y_n \\mid \\mathbf{w}, \\mathbf{x}_n, \\sigma^2) &= \\text{Normal}(y_n \\mid \\mathrm{NN}(\\mathbf{x}_n\\;;\\;\\mathbf{w}), \\sigma^2),\\end{aligned}\n",
        "\n",
        "where $\\mathrm{NN}$ is a neural network whose weights and biases form the latent variables $\\mathbf{w}$. Assume $\\sigma^2$ is a known variance.\n",
        "\n",
        "Define the prior on the weights and biases $\\mathbf{w}$ to be the standard normal\n",
        "\\begin{aligned} p(\\mathbf{w}) &= \\text{Normal}(\\mathbf{w} \\mid \\mathbf{0}, \\mathbf{I}).\\end{aligned}\n",
        "\n",
        "Let’s build the model in Edward. We define a 3-layer Bayesian neural network with $\\tanh$ nonlinearities."
      ]
    },
    {
      "metadata": {
        "id": "UWJfPE1kffdv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow_probability import edward2 as ed \n",
        "\n",
        "def neural_network(x):\n",
        "    h = tf.tanh(tf.matmul(x, W_0) + b_0)\n",
        "    h = tf.tanh(tf.matmul(h, W_1) + b_1)\n",
        "    h = tf.matmul(h, W_2) + b_2\n",
        "    return tf.reshape(h, [-1])\n",
        "\n",
        "N = 40  # number of data ponts\n",
        "D = 1   # number of features\n",
        "\n",
        "W_0 = ed.Normal(loc=tf.zeros([D, 10]), scale=tf.ones([D, 10]))\n",
        "W_1 = ed.Normal(loc=tf.zeros([10, 10]), scale=tf.ones([10, 10]))\n",
        "W_2 = ed.Normal(loc=tf.zeros([10, 1]), scale=tf.ones([10, 1]))\n",
        "b_0 = ed.Normal(loc=tf.zeros(10), scale=tf.ones(10))\n",
        "b_1 = ed.Normal(loc=tf.zeros(10), scale=tf.ones(10))\n",
        "b_2 = ed.Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
        "\n",
        "x = tf.cast(x_train, dtype=tf.float32) # Assuming x_train already exists\n",
        "# x = tf.random_uniform([N, D], minval=10, maxval=40, dtype=tf.float32) If x_train does not exist\n",
        "y = ed.Normal(loc=neural_network(x), scale=0.1 * tf.ones(N))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IcelgBLvfm2I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Classification Model Example:**\n",
        "\n",
        "Consider a data set $\\{(\\mathbf{x}_n, y_n)\\}$, where each data point comprises of features $\\mathbf{x}_n\\in\\mathbb{R}^D$ and output $y_n\\in{\\{0,1}\\}$. Define the likelihood for each data point as\n",
        "\\begin{aligned} p(y_n \\mid \\mathbf{w}, \\mathbf{x}_n) &= \\mathrm{NN}(\\mathbf{x}_n\\;;\\;\\mathbf{w})^{y_n} (1-\\mathrm{NN}(\\mathbf{x}_n\\;;\\;\\mathbf{w}))^{1-y_n},\\end{aligned}\n",
        "\n",
        "where $\\mathrm{NN}$ denotes the neural network's output with a logistic sigmoid as its activation function, i.e.\n",
        "$\\mathrm{NN}=sigma(a) = 1/(1+exp(-a))$. \n",
        "\n",
        "Weights and biases form the latent variables $\\mathbf{w}$. \n",
        "\n",
        "Define the prior on the weights and biases $\\mathbf{w}$ to be the standard normal\n",
        "\\begin{aligned} p(\\mathbf{w}) &= \\text{Normal}(\\mathbf{w} \\mid \\mathbf{0}, \\mathbf{I}).\\end{aligned}\n",
        "\n",
        "Let’s build this model in Edward. Again, we define a 3-layer Bayesian neural network with $\\tanh$ nonlinearities."
      ]
    },
    {
      "metadata": {
        "id": "2WqPOoQIjNHD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def neural_network_bi(x):\n",
        "    h = tf.tanh(tf.matmul(x, W_0) + b_0)\n",
        "    h = tf.tanh(tf.matmul(h, W_1) + b_1)\n",
        "    h = tf.nn.sigmoid(tf.matmul(h, W_2) + b_2)\n",
        "    return tf.reshape(h, [-1])\n",
        "\n",
        "N = 40  # number of data ponts\n",
        "D = 1   # number of features\n",
        "\n",
        "W_0 = ed.Normal(loc=tf.zeros([D, 10]), scale=tf.ones([D, 10]))\n",
        "W_1 = ed.Normal(loc=tf.zeros([10, 10]), scale=tf.ones([10, 10]))\n",
        "W_2 = ed.Normal(loc=tf.zeros([10, 1]), scale=tf.ones([10, 1]))\n",
        "b_0 = ed.Normal(loc=tf.zeros(10), scale=tf.ones(10))\n",
        "b_1 = ed.Normal(loc=tf.zeros(10), scale=tf.ones(10))\n",
        "b_2 = ed.Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
        "\n",
        "x = tf.cast(x_train, dtype=tf.float32) # Assuming x_train already exists\n",
        "# x = tf.random_uniform([N, D], minval=10, maxval=40, dtype=tf.float32) If x_train does not exist\n",
        "y = ed.Bernoulli(probs=neural_network_bi(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3qZ-7Tpvq59",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "Neal, R. M. (2012). Bayesian learning for neural networks (Vol. 118). Springer Science & Business Media."
      ]
    }
  ]
}